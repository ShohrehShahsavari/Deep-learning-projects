import tensorflow as tf
import numpy as np
from sklearn.datasets import fetch_california_housing
import torch
import torch.nn as nn
import matplotlib.pyplot as plt





import torch

x = torch.tensor(3.5, requires_grad= True)
y = x**2
y.backward()
print(x.data)
print(x.grad)


a = torch.tensor(4.)
b = torch.tensor(6.)
x = torch.tensor(3., requires_grad= True)
y = a*x**2 + b
y.backward()
print(x.data)
print(x.grad)
print(a.grad)


w = torch.tensor(4., requires_grad= True)
b = torch.tensor(6., requires_grad= True)
x = torch.tensor(3.)
y = w*x + b
y.backward()

print(w.grad)
print(b.grad)






def func(x):
    return x**2

xDatas=[]
x= torch.tensor(6. , requires_grad= True)
n= 10
eta= .1
for i in range (0, n):
    xDatas.append(x.data.item())
    func(x).backward()
    x.data -= eta * x.grad
    x.grad.zero_()
yDatas=[func(torch.tensor(x).item()) for x in xDatas]

# Plot of y=x**2
xList= torch.linspace(-10, 10, 300)
yList= func(xList)
plt.plot(xList, yList)
plt.scatter(xDatas, yDatas, color= 'red')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("Gradient Descent Visualization")
plt.show()
    



def func(x):
    return x**4 - 8*x**2 - 3*x


xDatas=[]
x= torch.tensor(-1. , requires_grad= True)
n= 1000
eta= .01
gradient_threshold= .01

for i in range (n):
    xDatas.append(x.data.item())
    func(x).backward()
    if abs(x.grad.item()) < gradient_threshold:
        print(f"Converged at step {i}: gradient = {x.grad.item():.6f}")
        break
    with torch.no_grad():
        x.data += eta * x.grad
        x.grad.zero_()
yDatas=[func(torch.tensor(x).item()) for x in xDatas]

# Plot of y=x**2
xList= torch.linspace(-3, 3, 3000)
yList= func(xList)
plt.plot(xList, yList, label= "f(x) = x^4 - 8*x^2 - 3*x")
plt.scatter(xDatas, yDatas, color= 'red')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("Gradient Descent Visualization")
plt.show()
    





class Neuron:
    def __init__(self, n, af='linear'):
        self.w= torch.randn(n, requires_grad= True)
        self.b= torch.randn(1, requires_grad= True)
        self.af= af
    def __call__(self, X):
        z= self.w @ X + self.b
        print(f'w: {self.w}')
        print(f'b: {self.b.item()}')
        print(f'z: {self.w}')
        if self.af == 'linear':
            return self.linear(z)
        elif self.af == 'step':
            return self.step(z)
        elif self.af == 'relu':
            return self.relu(z)
        elif self.af == 'sigmoid':
            return self.sigmoid(z)
        elif self.af == 'tanh':
            return self.tanh(z)
        else:
            raise ValueError(f'Unsopported activation function {self.af}')

    
    def linear(self, x):
        return x
    def step(self, x):
        return (x>0).float()
    def relu(self, x):
        return torch.relu(x)        #return torch.maximum(x, torch.tensor(0.0)) 
    def sigmoid(self, x):
        return torch.sigmoid(x)     #return 1 / (1 + torch.exp(-x))
    def tanh(self, x):
        return torch.tanh(x)


x= torch.tensor([1, 5, 8, .65, 6])
y = torch.tensor([.4])
neuron1 = Neuron(5,'tanh')
y_pred = neuron1(x)
mse_loss= (y_pred - y) ** 2
print(f'y_pred: {y_pred.item()}')
print(f'mse_loss: {mse_loss.item()}')





class Neuron:
    def __init__(self, n_x, n_y, af='linear'):
        self.w= torch.randn(n_y, n_x, requires_grad= True)
        self.b= torch.randn(n_y, requires_grad= True)
        self.af= af
    
    def forward(self, X):
        z= self.w @ X + self.b
        print(f'w: {self.w.detach().numpy()}')
        print(f'b: {self.b.detach().numpy()}')
        print(f'z: {z.detach().numpy()}')
        
        if self.af == 'linear':
            return self.linear(z)
        elif self.af == 'step':
            return self.step(z)
        elif self.af == 'relu':
            return self.relu(z)
        elif self.af == 'sigmoid':
            return self.sigmoid(z)
        elif self.af == 'tanh':
            return self.tanh(z)
        else:
            raise ValueError(f'Unsopported activation function {self.af}')        
    
    def __call__(self, X, y=None, lr=None):
        yp = self.forward(X)
        if y is not None:
            print(f"yt (y true) : {y.detach().numpy()}")
        print(f"yp (y predict) : {yp.detach().numpy()}")

        if y is not None and lr is not None:
            loss= self.compute_loss(yp, y)
            loss.backward()
            self.update_params(lr)
            return yp , loss
        else:
            return yp
            

    def compute_loss(self, yp, yt):
        return ((yp - yt)**2).mean()
        
    def linear(self, x):
        return x
    def step(self, x):
        return (x>0).float()
    def relu(self, x):
        return torch.relu(x)        #return torch.maximum(x, torch.tensor(0.0)) 
    def sigmoid(self, x):
        return torch.sigmoid(x)     #return 1 / (1 + torch.exp(-x))
    def tanh(self, x):
        return torch.tanh(x)

    def update_params(self, lr):
        with torch.no_grad():
            self.w -= lr * self.w.grad
            self.b -= lr * self.b.grad
            self.zero_grad()
    def zero_grad(self):
        if self.w.grad is not None:
            self.w.grad.zero_()
        if self.b.grad is not None:
            self.b.grad.zero_()        


x= torch.tensor([1, 5, 8, .65, 6, 10])
y = torch.tensor([6., 3.2])
neuron1 = Neuron(6, 2, 'linear')
y_pred , loss= neuron1(x, y, 0.1)



print(y_pred , loss)





import torch.nn as nn


# x= torch.tensor([1, 5, 8, .65, 6, 10])
# y = torch.tensor([6., 3.2])

x= torch.randn(200, 4)
y = torch.randn(200, 2)

model = nn.Linear(in_features=4, out_features=2)
print(f'Bias: {model.bias.detach().numpy()}\nWeights: {model.weight.detach().numpy()}')

yp= model(x)
# print(f'y predict: {yp.detach().numpy()}')

loss_fun = nn.MSELoss()
mse = loss_fun(yp, y)
print(f'MSE : {mse}')



from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

housing= fetch_california_housing()
m, n= housing.data.shape

X = torch.tensor(housing.data, dtype=torch.float32)
y = torch.tensor(housing.target, dtype=torch.float32).reshape(-1, 1)

scaler = StandardScaler()
X = torch.tensor(scaler.fit_transform(housing.data), dtype=torch.float32)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=42)


model = nn.Linear(n, 1)
loss_fun = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)

for epoch in range(100):
    yp = model(X_train)
    mse = loss_fun(yp, y_train)
    # print(f"{model.bias.detach().numpy()}  {model.weight.detach().numpy()}   {mse.item()}")
    # print(f"{mse.item()}")
    mse.backward()
    optimizer.step()
    optimizer.zero_grad()

y_predict = model(X_test)
l1_loss = nn.functional.l1_loss(y_predict, y_test)

l1_loss.item()





from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

housing= fetch_california_housing()
m, n= housing.data.shape

X = torch.tensor(housing.data, dtype=torch.float32)
y = torch.tensor(housing.target, dtype=torch.float32).reshape(-1, 1)

scaler = StandardScaler()
X = torch.tensor(scaler.fit_transform(housing.data), dtype=torch.float32)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=42)


model1 = nn.Linear(n, 7)
model2 = nn.Linear(7, 5)
model3 = nn.Linear(5, 1)
mlp = nn.Sequential(model1, model2, model3)
loss_fun = nn.MSELoss()
optimizer = torch.optim.SGD(mlp.parameters(), lr = 0.1)
loss_list=[]
for epoch in range(100):
    yp = mlp(X_train)
    mse = loss_fun(yp, y_train)
    loss_list.append(mse.item())
    # print(f"{model.bias.detach().numpy()}  {model.weight.detach().numpy()}   {mse.item()}")
    # print(f"{mse.item()}")
    mse.backward()
    optimizer.step()
    optimizer.zero_grad()

y_predict = mlp(X_test)
l1_loss = nn.functional.l1_loss(y_predict, y_test)

print(f"l1_loss: {l1_loss.item()}")

plt.plot(range(100), loss_list)
plt.xlabel(" epoch ")
plt.ylabel(" loss_function ")
plt.title("learning rate")
plt.show()





x= tf.Variable(3, name='x')
y= tf.Variable(4, name='y')
f= x*x*y + y + 2
print(f.numpy())


housing= fetch_california_housing()
m, n= housing.data.shape
housing_data_plus_bias= np.c_[np.ones((m, 1)), housing.data]
X= tf.constant(housing_data_plus_bias, dtype= tf.float32, name= 'X')
y= tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name= 'y')

# theta = (X^T * X)^(-1) * X^T * y
XT = tf.transpose(X)
theta = tf.matmul(tf.matmul(tf.linalg.inv(tf.matmul(XT, X)), XT), y)

# No session needed in TF 2.x
print(theta.numpy())


epochs = 1000
learning_rate = 0.01
X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')
y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name='y')
theta = tf.Variable(tf.random.uniform([n+1, 1], -1.0, 1.0), name='theta')

# Training loop
for epoch in range(epochs):
    with tf.GradientTape() as tape:
        y_pred = tf.matmul(X, theta, name='prediction')
        error = y_pred - y
        mse = tf.reduce_mean(tf.square(error), name='mse')
    
    # Manual gradient computation (as in your original code)
    gradient = 2/m * tf.matmul(tf.transpose(X), error)
    
    # Update theta using assign_sub (equivalent to theta = theta - learning_rate * gradient)
    theta.assign_sub(learning_rate * gradient)
    
    if epoch % 100 == 0:
        print('Epoch', epoch, 'MSE', mse.numpy())

best_theta = theta.numpy()
print("Best theta:", best_theta)



