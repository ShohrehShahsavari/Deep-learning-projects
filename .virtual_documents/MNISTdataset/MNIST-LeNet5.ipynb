import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras


import warnings
warnings.filterwarnings('ignore')





(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()


X_train = X_train.reshape(-1,28,28, 1).astype(np.float32) / 255.0
X_test = X_test.reshape(-1,28,28, 1).astype(np.float32) / 255.0  # Normalize


batch_size, height, width, channels = X_train.shape








print(f'X_train.shape {X_train.shape}')

inputs = keras.layers.Input(shape=(28,28,1))
conv1 = tf.nn.conv2d(inputs, tf.Variable(tf.random.normal(shape= (5, 5, channels, 6), dtype=np.float32)), strides=1, padding="SAME")
conv1 = tf.nn.tanh(conv1)
pool1 = tf.nn.avg_pool(conv1, ksize= [1, 2, 2, 1], strides=2, padding='VALID')
pool1 = tf.nn.tanh(pool1)
print(f"conv1 shape: {conv1.shape}")
print(f"pool1 shape: {pool1.shape}")

conv2 = tf.nn.conv2d(pool1, tf.Variable(tf.random.normal(shape= (5, 5, 6, 16), dtype=np.float32)), strides=1, padding='VALID')
conv2 = tf.nn.tanh(conv2)
pool2 = tf.nn.avg_pool(conv2, ksize= [1, 2, 2, 1], strides=2, padding='VALID')
pool2 = tf.nn.tanh(pool2)
print(f"conv2 shape: {conv2.shape}")
print(f"pool2 shape: {pool2.shape}")

conv3 = tf.nn.conv2d(pool2, tf.Variable(tf.random.normal(shape= (5, 5, 16, 120), dtype=np.float32)), strides=1, padding='VALID')
conv3 = tf.nn.tanh(conv3)
print(f"conv3 shape: {conv3.shape}")

flatten_layer = keras.layers.Flatten()(conv3)

layer1= keras.layers.Dense(84, activation= 'tanh')(flatten_layer)

output_layer= keras.layers.Dense(10,activation= 'softmax')(layer1)

model=keras.Model(inputs, output_layer)
model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train,y_train, epochs = 10, batch_size= 36, validation_data= (X_test, y_test))
# print(model.summary())

test_loss, test_acc= model.evaluate(X_test,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")





input_layer = keras.layers.Input(shape=(28,28,1))

C1_layer = keras.layers.Conv2D(filters=6, strides=1, kernel_size=(5,5), activation='tanh')(input_layer)
pool1_layer = keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(C1_layer)

C2_layer = keras.layers.Conv2D(filters=16, strides=1, kernel_size=(5,5), activation='tanh')(pool1_layer)
pool2_layer = keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(C2_layer)

C3_layer = keras.layers.Conv2D(filters=120, strides=1, kernel_size=(5,5), activation='tanh', padding='SAME')(pool2_layer)

flatten_layer= keras.layers.Flatten()(C3_layer)
hidden_layer1= keras.layers.Dense(84, activation= 'tanh')(flattSen_layer)

output_layer= keras.layers.Dense(10, activation= 'softmax')(hidden_layer1)

model=keras.Model(input_layer, output_layer)
model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

cnn_model = model.fit(X_train,y_train, epochs=10, batch_size=128, validation_data= (X_test, y_test)) 
#print(model.summary())

test_loss, test_acc= model.evaluate(X_test,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")


plt.plot(cnn_model.history['accuracy'], color='blue')
plt.plot(cnn_model.history['val_accuracy'], color='red')


plt.plot(cnn_model.history['loss'], color='blue')
plt.plot(cnn_model.history['val_loss'], color='red')


input_layer = keras.layers.Input(shape= (28, 28))
flatten_layer = keras.layers.Flatten()(input_layer)

# First hidden block with BatchNorm
hidden_layer1 = keras.layers.Dense(128)(flatten_layer)  # No activation here
hidden_layer1 = keras.layers.BatchNormalization()(hidden_layer1)  # Add BatchNorm
hidden_layer1 = keras.layers.Activation('relu')(hidden_layer1)  # Activation AFTER BatchNorm

# Second hidden block with BatchNorm  
hidden_layer2 = keras.layers.Dense(256)(hidden_layer1)  # No activation here
hidden_layer2 = keras.layers.BatchNormalization()(hidden_layer2)  # Add BatchNorm
hidden_layer2 = keras.layers.Activation('relu')(hidden_layer2)  # Activation AFTER BatchNorm

output_layer = keras.layers.Dense(10, activation = 'softmax')(hidden_layer2)

model= keras.Model(input_layer, output_layer)
# optimizer= ['adam', 'rmsprop', 'sgd']
# Or from tensorflow.keras.optimizers import SGD
# optimizer = SGD(learning_rate=0.01, momentum=0.9)
# loss= ['mean_squared_error', 'mean_absolute_error', 'huber_loss', 'mean_squared_logarithmic_error'] regression
# loss= ['binary_crossentropy', 'sparse_categorical_crossentropy', 'categorical_crossentropy'] classification
# metrics=['mse', 'mse'] regression
### clasification metrics?
# Accuracy metrics
# metrics=['accuracy']  # Auto-detects binary/multi-class
# metrics=['binary_accuracy']  # For binary classification
# metrics=['categorical_accuracy']  # For one-hot encoded multi-class
# metrics=['sparse_categorical_accuracy']  # For integer labels multi-class
# metrics=['top_k_categorical_accuracy']  # Top-k accuracy
# metrics=['sparse_top_k_categorical_accuracy']  # Top-k with integer labels

# # Precision/Recall/F1
# metrics=['precision']  # For binary
# metrics=['recall']  # For binary  
# metrics=['auc']  # Area Under ROC Curve
# metrics=['prc'] or metrics=['precision_at_recall']  # Precision-Recall Curve

model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])
model.fit(X_train, y_train, epochs= 10, batch_size= 32, validation_data = (X_test, y_test))


!pip install visualkeras


import sys
!{sys.executable} -m pip install visualkeras


import visualkeras
visualkeras.layered_view(cnn_model)



