import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras


import warnings
warnings.filterwarnings('ignore')





(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()


X_train = X_train.reshape(-1,28,28, 1).astype(np.float32) / 255.0
X_test = X_test.reshape(-1,28,28, 1).astype(np.float32) / 255.0  # Normalize


# Add padding to resize X.shape from (60000, 28, 28, 1) to (60000, 32, 32, 1)

# Calculate padding needed
pad_total = 32 - 28
pad_before = pad_total // 2
pad_after = pad_total - pad_before

# Apply padding: (top, bottom), (left, right), (0, 0) for channels
X_train_padded = np.pad(X_train, ((0, 0), (pad_before, pad_after), (pad_before, pad_after), (0, 0)), mode='constant', constant_values=0)
X_test_padded = np.pad(X_test, ((0, 0), (pad_before, pad_after), (pad_before, pad_after), (0, 0)), mode='constant', constant_values=0)


batch_size, height, width, channels = X_train.shape








print(f'X_train.shape {X_train.shape}')

inputs = keras.layers.Input(shape=(28,28,1))
conv1 = tf.nn.conv2d(inputs, tf.Variable(tf.random.normal(shape= (5, 5, channels, 6), dtype=np.float32)), strides=1, padding="SAME")
conv1 = tf.nn.tanh(conv1)
pool1 = tf.nn.avg_pool(conv1, ksize= [1, 2, 2, 1], strides=2, padding='VALID')
pool1 = tf.nn.tanh(pool1)
print(f"conv1 shape: {conv1.shape}")
print(f"pool1 shape: {pool1.shape}")

conv2 = tf.nn.conv2d(pool1, tf.Variable(tf.random.normal(shape= (5, 5, 6, 16), dtype=np.float32)), strides=1, padding='VALID')
conv2 = tf.nn.tanh(conv2)
pool2 = tf.nn.avg_pool(conv2, ksize= [1, 2, 2, 1], strides=2, padding='VALID')
pool2 = tf.nn.tanh(pool2)
print(f"conv2 shape: {conv2.shape}")
print(f"pool2 shape: {pool2.shape}")

conv3 = tf.nn.conv2d(pool2, tf.Variable(tf.random.normal(shape= (5, 5, 16, 120), dtype=np.float32)), strides=1, padding='VALID')
conv3 = tf.nn.tanh(conv3)
print(f"conv3 shape: {conv3.shape}")

flatten_layer = keras.layers.Flatten()(conv3)

layer1= keras.layers.Dense(84, activation= 'tanh')(flatten_layer)

output_layer= keras.layers.Dense(10,activation= 'softmax')(layer1)

model=keras.Model(inputs, output_layer)
model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train,y_train, epochs = 10, batch_size= 36, validation_data= (X_test, y_test))
# print(model.summary())

test_loss, test_acc= model.evaluate(X_test,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")





input_layer = keras.layers.Input(shape=(28,28,1))

C1_layer = keras.layers.Conv2D(filters=6, strides=1, kernel_size=(5,5), activation='tanh')(input_layer)
pool1_layer = keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(C1_layer)

C2_layer = keras.layers.Conv2D(filters=16, strides=1, kernel_size=(5,5), activation='tanh')(pool1_layer)
pool2_layer = keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(C2_layer)

C3_layer = keras.layers.Conv2D(filters=120, strides=1, kernel_size=(5,5), activation='tanh', padding='SAME')(pool2_layer)

flatten_layer= keras.layers.Flatten()(C3_layer)
hidden_layer1= keras.layers.Dense(84, activation= 'tanh')(flatten_layer)

output_layer= keras.layers.Dense(10, activation= 'softmax')(hidden_layer1)

model=keras.Model(input_layer, output_layer)
model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

LeNet5_model = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data= (X_test, y_test)) 
#print(model.summary())

test_loss, test_acc= model.evaluate(X_test,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")


plt.plot(LeNet5_model.history['accuracy'], color='blue')
plt.plot(LeNet5_model.history['val_accuracy'], color='red')
plt.title(" LetNet5 model Training Progress")
plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='best', fontsize=12)

plt.tight_layout()
plt.show()


plt.plot(LeNet5_model.history['loss'], color='blue')
plt.plot(LeNet5_model.history['val_loss'], color='red')

plt.title(" LetNet5 model Training Progress")
plt.legend(['Training loss', 'Validation loss'], loc='best', fontsize=12)

plt.tight_layout()
plt.show()


import visualkeras
visualkeras.layered_view(model)





input_layer = keras.layers.Input(shape=(32,32,1))

C1_layer = keras.layers.Conv2D(filters=32, strides=1, kernel_size=(3,3), activation='relu')(input_layer)
C1_layer = keras.layers.BatchNormalization()(C1_layer)
pool1_layer = keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(C1_layer)

C2_layer = keras.layers.Conv2D(filters=64, strides=1, kernel_size=(3,3), activation='relu')(pool1_layer)
C2_layer = keras.layers.BatchNormalization()(C2_layer)
pool2_layer = keras.layers.AveragePooling2D(pool_size=(2,2), strides=2)(C2_layer)


flatten_layer = keras.layers.Flatten()(pool2_layer)
hidden_layer1 = keras.layers.Dense(128, activation= 'relu')(flatten_layer)
hidden_layer1 = keras.layers.Dropout(0.5)(hidden_layer1)

output_layer= keras.layers.Dense(10, activation= 'softmax')(hidden_layer1)

cnn_model = keras.Model(input_layer, output_layer)
cnn_model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

modernLe_model = cnn_model.fit(X_train_padded, y_train, epochs=10, batch_size=128, validation_data= (X_test_padded, y_test)) 
#print(model.summary())

test_loss, test_acc= cnn_model.evaluate(X_test_padded,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")



plt.plot(modernLe_model.history['accuracy'], color ='blue')
plt.plot(modernLe_model.history['val_accuracy'], color ='red')


!pip install visualkeras


import sys
!{sys.executable} -m pip install visualkeras


import visualkeras
visualkeras.layered_view(cnn_model)



