import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras


import warnings
warnings.filterwarnings('ignore')





(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()


# with np.load('mnist.npz') as data:
#     X_train = data['x_train']
#     y_train = data['y_train']
#     X_test = data['x_test']
#     y_test = data['y_test']


 X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize


fig, axes= plt.subplots(nrows=4 , ncols=6 , figsize=(9,6))
for item in zip(axes.ravel(), X_train, y_train):
    ax, image, target= item
    ax.imshow(image, cmap= 'gray_r')
    ax.set_title(f'Label: {target}')
    ax.axis('off')
plt.tight_layout()





imput_layer= keras.layers.Input(shape=(28,28))
flatten_layer= keras.layers.Flatten()(imput_layer)
hidden_layer1= keras.layers.Dense(128, activation= 'relu')(flatten_layer)
hidden_layer2= keras.layers.Dense(256, activation= 'relu')(hidden_layer1)
output_layer= keras.layers.Dense(10,activation= 'softmax')(hidden_layer2)

model=keras.Model(imput_layer, output_layer)
model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train,y_train)
print(model.summary())

test_loss, test_acc= model.evaluate(X_test,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")


imput_layer= keras.layers.Input(shape=(28,28))
flatten_layer= keras.layers.Flatten()(imput_layer)
hidden_layer1= keras.layers.Dense(128, activation= 'relu')(flatten_layer)
hidden_layer2= keras.layers.Dense(256, activation= 'relu')(hidden_layer1)
output_layer= keras.layers.Dense(10,activation= 'softmax')(hidden_layer2)

model=keras.Model(imput_layer, output_layer)
model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train,y_train, epochs=10, batch_size=32)
#print(model.summary())

test_loss, test_acc= model.evaluate(X_test,y_test)
print(f" test loss: {test_loss}")
print(f" test accuracy: {test_acc}")





input_layer = keras.layers.Input(shape= (28, 28))
flatten_layer = keras.layers.Flatten()(input_layer)

# First hidden block with BatchNorm
hidden_layer1 = keras.layers.Dense(128)(flatten_layer)  # No activation here
hidden_layer1 = keras.layers.BatchNormalization()(hidden_layer1)  # Add BatchNorm
hidden_layer1 = keras.layers.Activation('relu')(hidden_layer1)  # Activation AFTER BatchNorm

# Second hidden block with BatchNorm  
hidden_layer2 = keras.layers.Dense(256)(hidden_layer1)  # No activation here
hidden_layer2 = keras.layers.BatchNormalization()(hidden_layer2)  # Add BatchNorm
hidden_layer2 = keras.layers.Activation('relu')(hidden_layer2)  # Activation AFTER BatchNorm

output_layer = keras.layers.Dense(10, activation = 'softmax')(hidden_layer2)

model= keras.Model(input_layer, output_layer)
# optimizer= ['adam', 'rmsprop', 'sgd']
# Or from tensorflow.keras.optimizers import SGD
# optimizer = SGD(learning_rate=0.01, momentum=0.9)
# loss= ['mean_squared_error', 'mean_absolute_error', 'huber_loss', 'mean_squared_logarithmic_error'] regression
# loss= ['binary_crossentropy', 'sparse_categorical_crossentropy', 'categorical_crossentropy'] classification
# metrics=['mse', 'mse'] regression
### clasification metrics?
# Accuracy metrics
# metrics=['accuracy']  # Auto-detects binary/multi-class
# metrics=['binary_accuracy']  # For binary classification
# metrics=['categorical_accuracy']  # For one-hot encoded multi-class
# metrics=['sparse_categorical_accuracy']  # For integer labels multi-class
# metrics=['top_k_categorical_accuracy']  # Top-k accuracy
# metrics=['sparse_top_k_categorical_accuracy']  # Top-k with integer labels

# # Precision/Recall/F1
# metrics=['precision']  # For binary
# metrics=['recall']  # For binary  
# metrics=['auc']  # Area Under ROC Curve
# metrics=['prc'] or metrics=['precision_at_recall']  # Precision-Recall Curve

model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])
model.fit(X_train, y_train, epochs= 10, batch_size= 32, validation_data = (X_test, y_test))





l1_reg = 0.001
l2_reg = 0.01

input_layer = keras.layers.Input(shape= (28, 28))
flatten_layer = keras.layers.Flatten()(input_layer)

# First hidden block
hidden_layer1 = keras.layers.Dense(128, kernel_regularizer= keras.regularizers.l1_l2(l1= l1_reg, l2= l2_reg), bias_regularizer= keras.regularizers.l2(l2_reg))(flatten_layer)
hidden_layer1 = keras.layers.BatchNormalization()(hidden_layer1)
hidden_layer1 = keras.layers.Activation('relu')(hidden_layer1)

# Second hidden block 
hidden_layer2 = keras.layers.Dense(256, kernel_regularizer= keras.regularizers.l1_l2(l1= l1_reg, l2= l2_reg), bias_regularizer= keras.regularizers.l2(l2_reg))(hidden_layer1)  # No activation here
hidden_layer2 = keras.layers.BatchNormalization()(hidden_layer2)  # Add BatchNorm
hidden_layer2 = keras.layers.Activation('relu')(hidden_layer2)  # Activation AFTER BatchNorm

output_layer = keras.layers.Dense(10, activation = 'softmax')(hidden_layer2)

model= keras.Model(input_layer, output_layer)

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss= 'sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])
model.fit(X_train, y_train, epochs= 10, batch_size= 32, validation_data = (X_test, y_test))





input_layer = keras.layers.Input(shape=(28, 28))
flatten_layer = keras.layers.Flatten()(input_layer)

hidden_layer1 = keras.layers.Dense(128)(flatten_layer)
hidden_layer1 = keras.layers.BatchNormalization()(hidden_layer1)
hidden_layer1 = keras.layers.Activation('relu')(hidden_layer1)
hidden_layer1 = keras.layers.Dropout(rate = 0.3)(hidden_layer1)

hidden_layer2 = keras.layers.Dense(256)(hidden_layer1)
hidden_layer2 = keras.layers.BatchNormalization()(hidden_layer2)
hidden_layer2 = keras.layers.Activation('relu')(hidden_layer2)
hidden_layer2 = keras.layers.Dropout(rate=0.3)(hidden_layer2)

output_layer = keras.layers.Dense(10, activation= 'softmax')(hidden_layer2)

dnn_model = keras.Model(input_layer, output_layer)
dnn_model.compile(optimizer= keras.optimizers.Adam(), loss = 'sparse_categorical_crossentropy', metrics= 'sparse_categorical_accuracy')
dnn_model.fit(X_train, y_train, epochs=10, batch_size= 32, validation_data=(X_test, y_test))


from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score
import tensorflow as tf
from tensorflow import keras
# from tensorflow.keras import layers

layer1 = keras.layers.Dense(300, activation= 'relu', input_shape= (X_train.shape[1],))
layer2 = keras.layers.Dense(100, activation= 'relu')
layer3 = keras.layers.Dense(10, activation= 'softmax')
dnn_model = keras.Sequential([layer1, layer2, layer3])
dnn_model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])
history = dnn_model.fit(X_train, y_train, batch_size=5, epochs= 5, validation_data= (X_test, y_test)) # epochs= 40000 // (len(X_train) // 50)
test_loss, test_accuracy = dnn_model.evaluate(X_test, y_test)
y_pred = dnn_model.predict(X_test)
y_pred_classes = tf.argmax(y_pred, axis=1)



