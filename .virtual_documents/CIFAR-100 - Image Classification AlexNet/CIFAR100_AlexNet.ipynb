import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow.keras as keras
import warnings
warnings.filterwarnings('ignore')
from keras.datasets import cifar100
import cv2


# ## Run following code in crm terminal
# C:\Users\ebsor\anaconda3\python.exe -m pip uninstall -y numpy pandas scipy scikit-learn numba shap tensorflow torch
# C:\Users\ebsor\anaconda3\python.exe -m pip install --no-cache-dir ^
# numpy==1.24.4 ^
# pandas==2.0.3 ^
# scipy==1.11.1 ^
# scikit-learn==1.3.2 ^
# numba==0.57.1 ^
# torch ^
# shap

# C:\Users\ebsor\anaconda3\python.exe -m pip install tensorflow==2.10.0 protobuf==3.19.6
# C:\Users\ebsor\anaconda3\python.exe -m pip install --no-cache-dir opencv-python==4.8.1.78





(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='coarse')


X_train.shape








# # Pre-allocate the array with the target shape
import gc
X_train_s = X_train[:2000]
y_train_s = y_train[:2000]

X_test_s  = X_test[:500]
y_test_s  = y_test[:500]


X_train_s_resized = np.zeros((2000, 224, 224, 3), dtype=np.uint8)
# Fill it incrementally
for i in range(len(X_train_s)):
    X_train_s_resized[i] = cv2.resize(X_train_s[i], (224, 224))
# Free original
del X_train_s
gc.collect()

# Pre-allocate the array with the target shape
X_test_s_resized = np.zeros((500, 224, 224, 3), dtype=np.uint8)
# Fill it incrementally
for i in range(len(X_test_s)):
    X_test_s_resized[i] = cv2.resize(X_test_s[i], (224, 224))
# Free original
del X_train
gc.collect()

# Convert to float32 and normalize
X_train_s = X_train_s_resized.astype(np.float32) / 255.0
X_test_s = X_test_s_resized.astype(np.float32) / 255.0

# Free uint8 arrays
del X_train_s_resized, X_test_s_resized
gc.collect()


### this code is ok but take a lot of time and it is very slow
input_layer = keras.layers.Input(shape=(224, 224, 3))

C1_layer = keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='SAME', activation='relu')(input_layer)
pool1_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C1_layer)

C2_layer = keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='SAME', activation='relu')(pool1_layer)
pool2_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C2_layer)

C3_layer = keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='SAME', activation='relu')(pool2_layer)
C4_layer = keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='SAME', activation='relu')(C3_layer)
C5_layer = keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='SAME', activation='relu')(C4_layer)

#flatten_layer = keras.layers.Flatten()(C5_layer)
flatten_layer = keras.layers.GlobalAveragePooling2D()(C5_layer)
Dens1_layer = keras.layers.Dense(4096, activation= 'relu')(flatten_layer)
Dens1_layer = keras.layers.Dropout(rate = 0.5)(Dens1_layer)

Dens2_layer = keras.layers.Dense(4096, activation= 'relu')(Dens1_layer)
Dens2_layer = keras.layers.Dropout(rate = 0.5)(Dens2_layer)

output_layer = keras.layers.Dense(20, activation= 'softmax')(Dens2_layer)

model = keras.Model(input_layer, output_layer)
model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),
    loss= 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)

callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,           # Reduce LR by half
        patience=3,           # Wait 3 epochs with no improvement
        min_lr=1e-6           # Minimum learning rate
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,          # Stop after 10 epochs without improvement
        restore_best_weights=True
    )
]
AlexNet_model = model.fit(X_train_s, y_train_s, epochs=25,batch_size= 64, validation_data=(X_test_s, y_test_s), callbacks=callbacks)


plt.plot(AlexNet_model.history['loss'], color='blue')
plt.plot(AlexNet_model.history['val_loss'], color='red')


plt.plot(AlexNet_model.history['sparse_categorical_accuracy'], color='blue')
plt.plot(AlexNet_model.history['val_sparse_categorical_accuracy'], color='red')


model.summary()





X_train = X_train[:2000].astype(np.float32) / 255.0
y_train = y_train[:2000]

X_test = X_test[:500].astype(np.float32) / 255.0
y_test = y_test[:500]


data_augmentation = keras.Sequential([
    keras.layers.Resizing(224, 224),
    keras.layers.RandomFlip('horizontal'),
    keras.layers.RandomTranslation(
        height_factor=0.1,
        width_factor=0.1
    )#,
    # keras.layers.RandomBrightness(factor=0.2),
    # keras.layers.RandomContrast(factor=0.2)

], name="data_augmentation")


input_layer = keras.layers.Input(shape=(32, 32, 3))
X = data_augmentation(input_layer)

C1_layer = keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='SAME')(X)
C1_layer = keras.layers.Activation('relu')(C1_layer)
pool1_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C1_layer)
# pool1_layer = keras.layers.BatchNormalization()(pool1_layer)

C2_layer = keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='SAME')(pool1_layer)
C2_layer = keras.layers.Activation('relu')(C2_layer)
pool2_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C2_layer)
# pool2_layer = keras.layers.BatchNormalization()(pool2_layer)

C3_layer = keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='SAME')(pool2_layer)
C3_layer = keras.layers.Activation('relu')(C3_layer)
# C3_layer = keras.layers.BatchNormalization()(C3_layer)

C4_layer = keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='SAME')(C3_layer)
C4_layer = keras.layers.Activation('relu')(C4_layer)
# C4_layer = keras.layers.BatchNormalization()(C4_layer)

C5_layer = keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='SAME')(C4_layer)
C5_layer = keras.layers.Activation('relu')(C5_layer)
# C5_layer = keras.layers.BatchNormalization()(C5_layer)

#flatten_layer = keras.layers.Flatten()(C5_layer)
flatten_layer = keras.layers.GlobalAveragePooling2D()(C5_layer)
Dens1_layer = keras.layers.Dense(4096, activation= 'relu')(flatten_layer)
Dens1_layer = keras.layers.Dropout(rate = 0.5)(Dens1_layer)
# Dens1_layer = keras.layers.BatchNormalization()(Dens1_layer)

Dens2_layer = keras.layers.Dense(4096, activation= 'relu')(Dens1_layer)
Dens2_layer = keras.layers.Dropout(rate = 0.5)(Dens2_layer)
# Dens2_layer = keras.layers.BatchNormalization()(Dens2_layer)

output_layer = keras.layers.Dense(20, activation= 'softmax')(Dens2_layer)

model = keras.Model(input_layer, output_layer)
model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0001),
    loss= 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)


model.summary()


callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,           # Reduce LR by half
        patience=3,           # Wait 3 epochs with no improvement
        min_lr=1e-6           # Minimum learning rate
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,          # Stop after 10 epochs without improvement
        restore_best_weights=True
    )
]
AlexNet_model = model.fit(X_train, y_train, epochs=25,batch_size= 64, validation_data=(X_test, y_test), callbacks=callbacks)


plt.plot(AlexNet_model.history['loss'], color='blue')
plt.plot(AlexNet_model.history['val_loss'], color='red')


plt.plot(AlexNet_model.history['sparse_categorical_accuracy'], color='blue')
plt.plot(AlexNet_model.history['val_sparse_categorical_accuracy'], color='red')





# ## Another way
# def preprocess(img, label):
#     img = tf.image.resize(img, (224, 224))
#     img = tf.cast(img, tf.float32) / 255.0
#     return img, label

# train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))
# train_ds = train_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)

# test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))
# test_ds = test_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)


train_ds_small = train_ds.take(20)   
test_ds_small  = test_ds.take(5)

input_layer = keras.layers.Input(shape=(224, 224, 3))

C1_layer = keras.layers.Conv2D(filters=96, kernel_size=(11, 11), strides=4, padding='SAME')(input_layer)
C1_layer = keras.layers.Activation('relu')(C1_layer)
pool1_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C1_layer)
# pool1_layer = keras.layers.BatchNormalization()(pool1_layer)

C2_layer = keras.layers.Conv2D(filters=256, kernel_size=(5, 5), strides=1, padding='SAME')(pool1_layer)
C2_layer = keras.layers.Activation('relu')(C2_layer)
pool2_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C2_layer)
# pool2_layer = keras.layers.BatchNormalization()(pool2_layer)

C3_layer = keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='SAME')(pool2_layer)
C3_layer = keras.layers.Activation('relu')(C3_layer)
# C3_layer = keras.layers.BatchNormalization()(C3_layer)

C4_layer = keras.layers.Conv2D(filters=384, kernel_size=(3, 3), strides=1, padding='SAME')(C3_layer)
C4_layer = keras.layers.Activation('relu')(C4_layer)
# C4_layer = keras.layers.BatchNormalization()(C4_layer)

C5_layer = keras.layers.Conv2D(filters=256, kernel_size=(3, 3), strides=1, padding='SAME')(C4_layer)
C5_layer = keras.layers.Activation('relu')(C5_layer)
# C5_layer = keras.layers.BatchNormalization()(C5_layer)

#flatten_layer = keras.layers.Flatten()(C5_layer)
flatten_layer = keras.layers.GlobalAveragePooling2D()(C5_layer)
Dens1_layer = keras.layers.Dense(4096, activation= 'relu')(flatten_layer)
Dens1_layer = keras.layers.Dropout(rate = 0.5)(Dens1_layer)
# Dens1_layer = keras.layers.BatchNormalization()(Dens1_layer)

Dens2_layer = keras.layers.Dense(4096, activation= 'relu')(Dens1_layer)
Dens2_layer = keras.layers.Dropout(rate = 0.5)(Dens2_layer)
# Dens2_layer = keras.layers.BatchNormalization()(Dens2_layer)

output_layer = keras.layers.Dense(20, activation= 'softmax')(Dens2_layer)

model = keras.Model(input_layer, output_layer)
model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),
    loss= 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)

callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,           # Reduce LR by half
        patience=3,           # Wait 3 epochs with no improvement
        min_lr=1e-6           # Minimum learning rate
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,          # Stop after 10 epochs without improvement
        restore_best_weights=True
    )
]
AlexNet_model = model.fit(train_ds_small, epochs=10, validation_data=test_ds_small, callbacks=callbacks)


plt.plot(AlexNet_model.history['loss'], color='blue')
plt.plot(AlexNet_model.history['val_loss'], color='red')


plt.plot(AlexNet_model.history['sparse_categorical_accuracy'], color='blue')
plt.plot(AlexNet_model.history['val_sparse_categorical_accuracy'], color='red')


model.summary()





X_train_s = X_train[:2000]
y_train_s = y_train[:2000]

X_test_s  = X_test[:500]
y_test_s  = y_test[:500]

X_train_s = X_train_s.astype(np.float32) / 255.0
X_test_s = X_test_s.astype(np.float32) / 255.0

X_train_s = np.array([cv2.resize(img, (140, 140)) for img in X_train_s])
X_test_s = np.array([cv2.resize(img, (140, 140)) for img in X_test_s])


input_layer = keras.layers.Input(shape=(140, 140, 3))

C1_layer = keras.layers.Conv2D(filters=32, kernel_size=(11, 11), strides=4, padding='SAME')(input_layer)
C1_layer = keras.layers.Activation('relu')(C1_layer)
pool1_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C1_layer)
pool1_layer = keras.layers.BatchNormalization()(pool1_layer)

C2_layer = keras.layers.Conv2D(filters=64, kernel_size=(5, 5), strides=1, padding='SAME')(pool1_layer)
C2_layer = keras.layers.Activation('relu')(C2_layer)
pool2_layer = keras.layers.MaxPooling2D(pool_size=(3, 3), strides=2, padding='VALID')(C2_layer)
pool2_layer = keras.layers.BatchNormalization()(pool2_layer)

C3_layer = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding='SAME')(pool2_layer)
C3_layer = keras.layers.Activation('relu')(C3_layer)
C3_layer = keras.layers.BatchNormalization()(C3_layer)

C4_layer = keras.layers.Conv2D(filters=128, kernel_size=(3, 3), strides=1, padding='SAME')(C3_layer)
C4_layer = keras.layers.Activation('relu')(C4_layer)
C4_layer = keras.layers.BatchNormalization()(C4_layer)

C5_layer = keras.layers.Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding='SAME')(C4_layer)
C5_layer = keras.layers.Activation('relu')(C5_layer)
C5_layer = keras.layers.BatchNormalization()(C5_layer)

#flatten_layer = keras.layers.Flatten()(C5_layer)
flatten_layer = keras.layers.GlobalAveragePooling2D()(C5_layer)
Dens1_layer = keras.layers.Dense(512, activation= 'relu')(flatten_layer)
Dens1_layer = keras.layers.Dropout(rate = 0.5)(Dens1_layer)
# Dens1_layer = keras.layers.BatchNormalization()(Dens1_layer)

Dens2_layer = keras.layers.Dense(512, activation= 'relu')(Dens1_layer)
Dens2_layer = keras.layers.Dropout(rate = 0.5)(Dens2_layer)
Dens2_layer = keras.layers.BatchNormalization()(Dens2_layer)

output_layer = keras.layers.Dense(20, activation= 'softmax')(Dens2_layer)

model = keras.Model(input_layer, output_layer)
model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-7),
    loss= 'sparse_categorical_crossentropy',
    metrics=['sparse_categorical_accuracy']
)

callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,           # Reduce LR by half
        patience=3,           # Wait 3 epochs with no improvement
        min_lr=1e-6           # Minimum learning rate
    ),
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=10,          # Stop after 10 epochs without improvement
        restore_best_weights=True
    )
]
AlexNet_model = model.fit(X_train_s, y_train_s, epochs=10, validation_data=(X_test_s, y_test_s), callbacks=callbacks)


plt.plot(AlexNet_model.history['loss'], color='blue')
plt.plot(AlexNet_model.history['val_loss'], color='red')
plt.title('AlexNet Training progress 2000*140*140*3')
plt.legend(['Training loss', 'Validation loss'], loc='best', fontsize=12)
plt.tight_layout()
plt.show()


plt.plot(AlexNet_model.history['sparse_categorical_accuracy'], color='green')
plt.plot(AlexNet_model.history['val_sparse_categorical_accuracy'], color='yellow')
plt.title('AlexNet Training progress 2000*140*140*3')
plt.legend(['Training accuracy', 'Validation accuracy'], loc='best', fontsize=12)
plt.tight_layout()
plt.show()


model.summary()


import visualkeras
visualkeras.layered_view(model)



